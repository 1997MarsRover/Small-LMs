{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMWvtbUF6rRqb3B2DADHjrh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/1997MarsRover/Small-LMs/blob/main/tokenization/BytePair-encoding/bpe_enc_dec.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Here is Simple implementation and training of BPE\n",
        "\n",
        "The *get_stats* func takes a vocabulary dictionary and computes the frequency of consecutive byte pairs across all words in the vocabulary.  "
      ],
      "metadata": {
        "id": "X-Mxu_pXL8o0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "url = \"https://www.reedbeta.com/blog/reading-veach-thesis-2/\"\n",
        "response = requests.get(url)\n",
        "soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "# Extract the text content from specific HTML tags\n",
        "article_text = soup.find(\"article\").get_text()\n",
        "print(article_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GPxsAGdgSNiS",
        "outputId": "896a67f5-0301-4cfe-a349-9382d4f2ecbd"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Reading Veach‚Äôs Thesis, Part¬†2\n",
            "February 25, 2023 ¬∑ Graphics, Math ¬∑ Comments\n",
            "\n",
            "In this post, we‚Äôre continuing to read Eric Veach‚Äôs doctoral thesis.\n",
            "In our last installment, we covered the first half of the\n",
            "thesis, dealing with theoretical foundations for Monte Carlo rendering. This time\n",
            "we‚Äôre tackling chapters 8‚Äì9, including one of the key algorithms this thesis is famous for:\n",
            "multiple importance sampling. Without further ado, let‚Äôs tuck in!\n",
            "As before, this isn‚Äôt going to be a comprehensive review of everything in the thesis‚Äîit‚Äôs just a\n",
            "selection of things that made me go ‚Äúoh, that‚Äôs cool‚Äù, or ‚Äúhuh! I didn‚Äôt know that‚Äù.\n",
            "\n",
            "\n",
            "\n",
            "Path-Space Integrals\n",
            "Non-Local Path Sampling\n",
            "Extended Light Path Expressions\n",
            "Multiple Importance Sampling\n",
            "The Balance Heuristic\n",
            "The Power Heuristic\n",
            "MIS Examples\n",
            "\n",
            "\n",
            "Conclusion\n",
            "\n",
            "\n",
            "Path-Space Integrals\n",
            "We usually see the rendering equation expressed as a fixed-point integral equation. The radiance\n",
            "field $L$ appears on both sides:\n",
            "$$\n",
            "    L = L_e + \\int L \\, f \\, |\\cos\\theta| \\, \\mathrm{d}\\omega\n",
            "$$\n",
            "There are some theorems showing that we can solve this as an infinite series:\n",
            "$$\n",
            "    L = L_e + TL_e + T^2 L_e + \\cdots\n",
            "$$\n",
            "where $T$ is an operator representing the integral over surfaces with their BSDFs. This series\n",
            "constructs the solution\n",
            "bounce-by-bounce: first directly emitted light, then light that‚Äôs been scattered once, then\n",
            "scattered twice, and so on.\n",
            "The trouble is, this series contains a separate integral for each possible path length.\n",
            "For the methods Veach is going to deploy later, he needs to be able to\n",
            "combine paths of all lengths in a single Monte Carlo estimator. In Chapter 8, he\n",
            "reformulates the rendering equation as an integral over a ‚Äúspace‚Äù of all possible paths:\n",
            "$$\n",
            "    L = \\int L_e \\, f \\, \\mathrm{d}\\mu\n",
            "$$\n",
            "The idea is that now we‚Äôre integrating a new kind of\n",
            "‚Äúvariable‚Äù, which ranges over all paths (of any length) in the scene. Here,\n",
            "$f$ stands for the throughput along a whole path, and $L_e$ for the emitted light injected at its\n",
            "beginning.\n",
            "By itself, this doesn‚Äôt really simplify anything; we‚Äôve just moved the complexity from the rendering\n",
            "equation to the definition of the path space over which we‚Äôre integrating. This is a funny kind of\n",
            "‚Äúspace‚Äù that actually consists of a disjoint union\n",
            "of an infinite sequence of subspaces, one for each possible path length. Those subspaces even have\n",
            "different dimensionalities, which is extra weird! But with Lebesgue measure theory, this is a legit\n",
            "space that can be integrated over in a mathematically rigorous way.\n",
            "This sets us up for talking about probability distributions over all paths, combining different path\n",
            "sampling methods in an unbiased way, and so forth‚Äîwhich will be crucial in the following chapters.\n",
            "The path-integral formulation of the rendering equation has also become quite popular in light\n",
            "transport theory papers today.\n",
            "Non-Local Path Sampling\n",
            "Veach gives an intriguing example of a potential new path sampling approach that‚Äôs facilitated by the\n",
            "path-integral formulation. Usually, paths are constructed incrementally starting from one end, by\n",
            "shooting a ray toward the next path vertex. But in the presence of specular surfaces such as a\n",
            "planar mirror, you could also algebraically solve for a point on the mirror\n",
            "that will connect two existing path vertices (say, one from a camera subpath and one from a light\n",
            "subpath). Even more exotically, we could consider solving for chains of multiple specular scattering\n",
            "events to connect a given pair of endpoints.\n",
            "Veach calls this ‚Äúnon-local‚Äù path sampling, because it looks at vertices that aren‚Äôt just\n",
            "adjacent to each other on the path, but farther apart.\n",
            "Veach merely sketches this idea and remarks that it could be useful. Since then, non-local\n",
            "sampling ideas have been researched in the manifold exploration\n",
            "family of techniques, such as Manifold Next-Event Estimation\n",
            "and Specular Manifold Sampling.\n",
            "Extended Light Path Expressions\n",
            "You may have seen ‚Äúregular expression‚Äù syntax describing the vertices of paths, like $LS^*DE$\n",
            "and suchlike. In this notation, $L$ stands for a light source, $S$ for a (Dirac) specular scattering\n",
            "event, $D$ a diffuse (or glossy) scattering event, and $E$ for the camera/eye.\n",
            "It‚Äôs a concise way to classify which kinds of paths are handled by different techniques. These\n",
            "‚Äúlight path expressions‚Äù are widely used in the literature, as well as in production renderers to\n",
            "split off different lighting components into separate framebuffers.\n",
            "Veach describes an extension to this notation in which extra $D$ and $S$ symbols are added to\n",
            "denote the continuity or discreteness of lights and cameras, in both position and directionality.\n",
            "For example, a point light (positionally ‚Äúspecular‚Äù) that radiates in all directions (‚Äúdiffuse‚Äù)\n",
            "would be denoted $LSD$. A punctual directional light would be $LDS$, and an area light\n",
            "would be $LDD$. The camera is described likewise, but in the opposite order: $DSE$ is a pinhole\n",
            "camera, while $DDE$ is a camera with a physical lens area. These substrings are used as prefixes and\n",
            "suffixes for what he calls ‚Äúfull-path‚Äù regular expressions.\n",
            "There‚Äôs a certain elegance to this idea, but I have to admit I found it confusing in practice, even\n",
            "after reading several chapters using these extended expressions. I had to keep looking\n",
            "up which symbol was the position and which was the direction, and stopping to think about what those\n",
            "labels mean in the context of a light source or camera.\n",
            "This extended syntax doesn‚Äôt seem to have been adopted by much later literature, but I did see it\n",
            "used in the Path Space Regularization\n",
            "paper by Kaplanyan and Dachsbacher. They also print the light and camera substrings in different\n",
            "colors, to improve their readability.\n",
            "Multiple Importance Sampling\n",
            "Alright, now we‚Äôre getting into the real meat of Veach‚Äôs thesis! In a sense, all the foregoing\n",
            "material was just setup and preparation for the last three chapters, which contain the thesis‚Äôs major\n",
            "original contributions.\n",
            "I‚Äôll assume you‚Äôre familiar with the basic ideas of multiple importance sampling, the balance heuristic,\n",
            "and the power heuristic. If you need a refresher, here‚Äôs the relevant section of PBR.\n",
            "The Balance Heuristic\n",
            "There are some great insights here about the interpretation of the balance heuristic that I\n",
            "hadn‚Äôt seen before. Using the balance heuristic to combine samples from a collection of\n",
            "probability distributions $p_i(x)$ (e.g., light source sampling and BSDF sampling)\n",
            "turns out to be equivalent to sampling from a single distribution, whose probability\n",
            "density is the average of all the constituent ones:\n",
            "$$\n",
            "p_\\text{mis}(x) = \\frac{1}{N} \\sum_i p_i(x)\n",
            "$$\n",
            "Intuitively, this is useful because the combined distribution inherits all of the peaks of the\n",
            "distributions contributing to it. If one sampling strategy is ‚Äúgood at‚Äù sampling a certain region\n",
            "of the integration domain, its $p_i(x)$ will tend to have a peak in that region. When several PDFs\n",
            "are averaged together, the resulting distribution has peaks (albeit smaller ones) everywhere any of\n",
            "the included strategies has a peak.\n",
            "As an illustration, here are two fictious ‚ÄúPDFs‚Äù I made up, and their average:\n",
            "\n",
            "\n",
            "\n",
            "The third curve, which simulates MIS with the balance heuristic, combines the peaks of\n",
            "the first two.\n",
            "Here‚Äôs all three curves together:\n",
            "\n",
            "So, the balance heuristic combines the strengths of the sampling strategies within it:\n",
            "it‚Äôs ‚Äúpretty good at‚Äù sampling all the regions that any of the constitutent strategies are ‚Äúgood at‚Äù.\n",
            "A corollary of this fact is that the balance heuristic will assign a given path the same\n",
            "contribution weight no matter which strategy generated it. This isn‚Äôt the case\n",
            "for other MIS weighting functions, such as the power heuristic.\n",
            "The Power Heuristic\n",
            "The power heuristic doesn‚Äôt have quite such a tidy interpretation; it‚Äôs not equivalent to sampling\n",
            "any single distribution. It intuitively does something similar to the balance heuristic, but also\n",
            "‚Äúsharpens‚Äù the weights, making small contributions smaller and large ones larger.\n",
            "According to\n",
            "Veach, this is helpful to reduce variance in areas where one of the included strategies is\n",
            "already a very close match for the integrand. In those cases, MIS isn‚Äôt really needed, and the\n",
            "balance heuristic can actually make things worse. The power heuristic makes things less worse.\n",
            "There‚Äôs a great graph in the thesis (Figure 9.10) showing actual variance measurements for light\n",
            "source sampling, BSDF sampling, and the two combined with the balance heuristic or the power heuristic:\n",
            "\n",
            "These are plotted logarithmically over several orders of magnitude in surface roughness, so they\n",
            "give some nice concrete evidence about the efficacy of MIS in reducing variance across a wide\n",
            "range of shading situations.\n",
            "MIS Examples\n",
            "We‚Äôve all seen that classic MIS showcase image, with the different light source sizes versus material\n",
            "roughnesses. That comes from this thesis, of course!\n",
            "Here‚Äôs a neat Shadertoy rendition of it, created by Maxwell Planck:\n",
            "\n",
            "Light source samples are color-coded red, and BSDF samples are green; this is a nice way to\n",
            "visualize how the two get weighted differently across the image.\n",
            "However, I was interested to see that Veach also has a second demo scene, which I haven‚Äôt come across\n",
            "before. It‚Äôs simpler and less ‚Äúpretty‚Äù than the more famous one above,\n",
            "but in my mind it demonstrates the value of MIS even more starkly.\n",
            "This scene just consists of a large emissive surface at right angles to a diffuse surface:\n",
            "\n",
            "(Shadertoy here, which I adapted from Planck‚Äôs.)\n",
            "Depending how far you are from the light, either BSDF sampling or light source sampling is more\n",
            "effective at estimating the illumination. So, you don‚Äôt even need a whole range of material roughnesses\n",
            "to benefit from MIS; area lights and diffuse walls are enough!\n",
            "Conclusion\n",
            "I‚Äôve known about multiple importance sampling for a long time, but I never felt like I quite\n",
            "got my head around it. I had the idea that it was something about shifting weight toward whichever\n",
            "sampling method gives you the ‚Äúhighest quality‚Äù samples in a given region, but it always\n",
            "seemed a little magical to me how you could determine that from purely local information (the pdfs\n",
            "at a single sample point).\n",
            "I‚Äôm glad I took the time to read through Veach‚Äôs own explanation of this, as it goes into a lot more\n",
            "detail about the meaning and intuition behind the balance heuristic. I have a much better\n",
            "understanding of how and why it works, now.\n",
            "One thing I didn‚Äôt get to address here (because I didn‚Äôt have much useful to say\n",
            "about it) was the optimality(-ish) proofs Veach gives. There are a few theorems proved in this\n",
            "chapter that roughly say something like ‚Äúthis heuristic might not be the best one, but it‚Äôs not\n",
            "that far behind the best one‚Äù. I‚Äôd like to contextualize these results better (what justifies saying\n",
            "it‚Äôs ‚Äúnot that far‚Äù?), but I haven‚Äôt yet found the right angle.\n",
            "The last couple chapters in the thesis are about bidirectional path tracing and Metropolis light\n",
            "transport. This post has stretched long enough, so those will have to wait for another time!\n",
            "\n",
            "\n",
            "Tweet\n",
            "\n",
            "\n",
            "Reading Veach‚Äôs Thesis \n",
            "\n",
            "Comments on ‚ÄúReading Veach‚Äôs Thesis, Part¬†2‚Äù\n",
            "\n",
            "\n",
            "Please enable JavaScript to view the comments powered by Disqus.\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "NMA5XGT-LEnm"
      },
      "outputs": [],
      "source": [
        "tokens = one_line_text.encode(\"utf-8\") # raw bytes\n",
        "tokens = list(map(int, tokens))"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "lines = [line.strip() for line in article_text.splitlines() if line.strip()]\n",
        "one_line_text = ' '.join(lines)"
      ],
      "metadata": {
        "id": "RmKy1aIETzaD"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "one_line_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 712
        },
        "id": "QZ2g2uL6URpZ",
        "outputId": "d30463b6-154f-4a17-9eba-0bc2fca42acb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Reading Veach‚Äôs Thesis, Part\\xa02 February 25, 2023 ¬∑ Graphics, Math ¬∑ Comments In this post, we‚Äôre continuing to read Eric Veach‚Äôs doctoral thesis. In our last installment, we covered the first half of the thesis, dealing with theoretical foundations for Monte Carlo rendering. This time we‚Äôre tackling chapters 8‚Äì9, including one of the key algorithms this thesis is famous for: multiple importance sampling. Without further ado, let‚Äôs tuck in! As before, this isn‚Äôt going to be a comprehensive review of everything in the thesis‚Äîit‚Äôs just a selection of things that made me go ‚Äúoh, that‚Äôs cool‚Äù, or ‚Äúhuh! I didn‚Äôt know that‚Äù. Path-Space Integrals Non-Local Path Sampling Extended Light Path Expressions Multiple Importance Sampling The Balance Heuristic The Power Heuristic MIS Examples Conclusion Path-Space Integrals We usually see the rendering equation expressed as a fixed-point integral equation. The radiance field $L$ appears on both sides: $$ L = L_e + \\\\int L \\\\, f \\\\, |\\\\cos\\\\theta| \\\\, \\\\mathrm{d}\\\\omega $$ There are some theorems showing that we can solve this as an infinite series: $$ L = L_e + TL_e + T^2 L_e + \\\\cdots $$ where $T$ is an operator representing the integral over surfaces with their BSDFs. This series constructs the solution bounce-by-bounce: first directly emitted light, then light that‚Äôs been scattered once, then scattered twice, and so on. The trouble is, this series contains a separate integral for each possible path length. For the methods Veach is going to deploy later, he needs to be able to combine paths of all lengths in a single Monte Carlo estimator. In Chapter 8, he reformulates the rendering equation as an integral over a ‚Äúspace‚Äù of all possible paths: $$ L = \\\\int L_e \\\\, f \\\\, \\\\mathrm{d}\\\\mu $$ The idea is that now we‚Äôre integrating a new kind of ‚Äúvariable‚Äù, which ranges over all paths (of any length) in the scene. Here, $f$ stands for the throughput along a whole path, and $L_e$ for the emitted light injected at its beginning. By itself, this doesn‚Äôt really simplify anything; we‚Äôve just moved the complexity from the rendering equation to the definition of the path space over which we‚Äôre integrating. This is a funny kind of ‚Äúspace‚Äù that actually consists of a disjoint union of an infinite sequence of subspaces, one for each possible path length. Those subspaces even have different dimensionalities, which is extra weird! But with Lebesgue measure theory, this is a legit space that can be integrated over in a mathematically rigorous way. This sets us up for talking about probability distributions over all paths, combining different path sampling methods in an unbiased way, and so forth‚Äîwhich will be crucial in the following chapters. The path-integral formulation of the rendering equation has also become quite popular in light transport theory papers today. Non-Local Path Sampling Veach gives an intriguing example of a potential new path sampling approach that‚Äôs facilitated by the path-integral formulation. Usually, paths are constructed incrementally starting from one end, by shooting a ray toward the next path vertex. But in the presence of specular surfaces such as a planar mirror, you could also algebraically solve for a point on the mirror that will connect two existing path vertices (say, one from a camera subpath and one from a light subpath). Even more exotically, we could consider solving for chains of multiple specular scattering events to connect a given pair of endpoints. Veach calls this ‚Äúnon-local‚Äù path sampling, because it looks at vertices that aren‚Äôt just adjacent to each other on the path, but farther apart. Veach merely sketches this idea and remarks that it could be useful. Since then, non-local sampling ideas have been researched in the manifold exploration family of techniques, such as Manifold Next-Event Estimation and Specular Manifold Sampling. Extended Light Path Expressions You may have seen ‚Äúregular expression‚Äù syntax describing the vertices of paths, like $LS^*DE$ and suchlike. In this notation, $L$ stands for a light source, $S$ for a (Dirac) specular scattering event, $D$ a diffuse (or glossy) scattering event, and $E$ for the camera/eye. It‚Äôs a concise way to classify which kinds of paths are handled by different techniques. These ‚Äúlight path expressions‚Äù are widely used in the literature, as well as in production renderers to split off different lighting components into separate framebuffers. Veach describes an extension to this notation in which extra $D$ and $S$ symbols are added to denote the continuity or discreteness of lights and cameras, in both position and directionality. For example, a point light (positionally ‚Äúspecular‚Äù) that radiates in all directions (‚Äúdiffuse‚Äù) would be denoted $LSD$. A punctual directional light would be $LDS$, and an area light would be $LDD$. The camera is described likewise, but in the opposite order: $DSE$ is a pinhole camera, while $DDE$ is a camera with a physical lens area. These substrings are used as prefixes and suffixes for what he calls ‚Äúfull-path‚Äù regular expressions. There‚Äôs a certain elegance to this idea, but I have to admit I found it confusing in practice, even after reading several chapters using these extended expressions. I had to keep looking up which symbol was the position and which was the direction, and stopping to think about what those labels mean in the context of a light source or camera. This extended syntax doesn‚Äôt seem to have been adopted by much later literature, but I did see it used in the Path Space Regularization paper by Kaplanyan and Dachsbacher. They also print the light and camera substrings in different colors, to improve their readability. Multiple Importance Sampling Alright, now we‚Äôre getting into the real meat of Veach‚Äôs thesis! In a sense, all the foregoing material was just setup and preparation for the last three chapters, which contain the thesis‚Äôs major original contributions. I‚Äôll assume you‚Äôre familiar with the basic ideas of multiple importance sampling, the balance heuristic, and the power heuristic. If you need a refresher, here‚Äôs the relevant section of PBR. The Balance Heuristic There are some great insights here about the interpretation of the balance heuristic that I hadn‚Äôt seen before. Using the balance heuristic to combine samples from a collection of probability distributions $p_i(x)$ (e.g., light source sampling and BSDF sampling) turns out to be equivalent to sampling from a single distribution, whose probability density is the average of all the constituent ones: $$ p_\\\\text{mis}(x) = \\\\frac{1}{N} \\\\sum_i p_i(x) $$ Intuitively, this is useful because the combined distribution inherits all of the peaks of the distributions contributing to it. If one sampling strategy is ‚Äúgood at‚Äù sampling a certain region of the integration domain, its $p_i(x)$ will tend to have a peak in that region. When several PDFs are averaged together, the resulting distribution has peaks (albeit smaller ones) everywhere any of the included strategies has a peak. As an illustration, here are two fictious ‚ÄúPDFs‚Äù I made up, and their average: The third curve, which simulates MIS with the balance heuristic, combines the peaks of the first two. Here‚Äôs all three curves together: So, the balance heuristic combines the strengths of the sampling strategies within it: it‚Äôs ‚Äúpretty good at‚Äù sampling all the regions that any of the constitutent strategies are ‚Äúgood at‚Äù. A corollary of this fact is that the balance heuristic will assign a given path the same contribution weight no matter which strategy generated it. This isn‚Äôt the case for other MIS weighting functions, such as the power heuristic. The Power Heuristic The power heuristic doesn‚Äôt have quite such a tidy interpretation; it‚Äôs not equivalent to sampling any single distribution. It intuitively does something similar to the balance heuristic, but also ‚Äúsharpens‚Äù the weights, making small contributions smaller and large ones larger. According to Veach, this is helpful to reduce variance in areas where one of the included strategies is already a very close match for the integrand. In those cases, MIS isn‚Äôt really needed, and the balance heuristic can actually make things worse. The power heuristic makes things less worse. There‚Äôs a great graph in the thesis (Figure 9.10) showing actual variance measurements for light source sampling, BSDF sampling, and the two combined with the balance heuristic or the power heuristic: These are plotted logarithmically over several orders of magnitude in surface roughness, so they give some nice concrete evidence about the efficacy of MIS in reducing variance across a wide range of shading situations. MIS Examples We‚Äôve all seen that classic MIS showcase image, with the different light source sizes versus material roughnesses. That comes from this thesis, of course! Here‚Äôs a neat Shadertoy rendition of it, created by Maxwell Planck: Light source samples are color-coded red, and BSDF samples are green; this is a nice way to visualize how the two get weighted differently across the image. However, I was interested to see that Veach also has a second demo scene, which I haven‚Äôt come across before. It‚Äôs simpler and less ‚Äúpretty‚Äù than the more famous one above, but in my mind it demonstrates the value of MIS even more starkly. This scene just consists of a large emissive surface at right angles to a diffuse surface: (Shadertoy here, which I adapted from Planck‚Äôs.) Depending how far you are from the light, either BSDF sampling or light source sampling is more effective at estimating the illumination. So, you don‚Äôt even need a whole range of material roughnesses to benefit from MIS; area lights and diffuse walls are enough! Conclusion I‚Äôve known about multiple importance sampling for a long time, but I never felt like I quite got my head around it. I had the idea that it was something about shifting weight toward whichever sampling method gives you the ‚Äúhighest quality‚Äù samples in a given region, but it always seemed a little magical to me how you could determine that from purely local information (the pdfs at a single sample point). I‚Äôm glad I took the time to read through Veach‚Äôs own explanation of this, as it goes into a lot more detail about the meaning and intuition behind the balance heuristic. I have a much better understanding of how and why it works, now. One thing I didn‚Äôt get to address here (because I didn‚Äôt have much useful to say about it) was the optimality(-ish) proofs Veach gives. There are a few theorems proved in this chapter that roughly say something like ‚Äúthis heuristic might not be the best one, but it‚Äôs not that far behind the best one‚Äù. I‚Äôd like to contextualize these results better (what justifies saying it‚Äôs ‚Äúnot that far‚Äù?), but I haven‚Äôt yet found the right angle. The last couple chapters in the thesis are about bidirectional path tracing and Metropolis light transport. This post has stretched long enough, so those will have to wait for another time! Tweet Reading Veach‚Äôs Thesis Comments on ‚ÄúReading Veach‚Äôs Thesis, Part\\xa02‚Äù Please enable JavaScript to view the comments powered by Disqus.'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *merge_vocab* function merges a given byte pair into a new token in the vocabulary. It uses regular expressions to replace the byte pair with the merged token across all words in the vocabulary."
      ],
      "metadata": {
        "id": "oLnYf6cmMt3D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_stats(ids):\n",
        "    counts = {}\n",
        "    for pair in zip(ids, ids[1:]):\n",
        "        counts[pair] = counts.get(pair, 0) + 1\n",
        "    return counts\n",
        "\n",
        "def merge(ids, pair, idx):\n",
        "  newids = []\n",
        "  i = 0\n",
        "  while i < len(ids):\n",
        "    if i < len(ids) - 1 and ids[i] == pair[0] and ids[i+1] == pair[1]:\n",
        "      newids.append(idx)\n",
        "      i += 2\n",
        "    else:\n",
        "      newids.append(ids[i])\n",
        "      i += 1\n",
        "  return newids\n",
        "\n",
        "# ---\n",
        "vocab_size = 276 # the desired final vocabulary size\n",
        "num_merges = vocab_size - 256\n",
        "ids = list(tokens) # copy so we don't destroy the original list"
      ],
      "metadata": {
        "id": "MzaCEkoELpH_"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "The *bpe* function is the main function that performs the Byte Pair Encoding algorithm. It takes a vocabulary dictionary and the desired number of merge operations (num_merges).\n",
        "\n",
        "- In each iteration, it calls *get_stats* to find the most frequent byte pair.\n",
        "- If there are no more pairs to merge, it breaks out of the loop.\n",
        "- Otherwise, it finds the most frequent byte pair (best) and calls *merge* func to merge it into a new token, updating the vocabulary.\n",
        "- After the specified number of merges, it returns the final merged vocabulary."
      ],
      "metadata": {
        "id": "PN4HjA-CM2TJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def bpe(ids, num_merges):\n",
        "    merges = {} # (int, int) -> int\n",
        "    for i in range(num_merges):\n",
        "        stats = get_stats(ids)\n",
        "        pair = max(stats, key=stats.get)\n",
        "        idx = 256 + i\n",
        "        print(f\"merging {pair} into a new token {idx}\")\n",
        "        ids = merge(ids, pair, idx)\n",
        "        merges[pair] = idx\n",
        "\n",
        "    return merges\n",
        "\n",
        "\n",
        "\n",
        "merged_vocab = bpe(ids, num_merges=10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rffBijFOLuAL",
        "outputId": "5ee95d2a-3852-4933-b9d7-4abd56bb303d"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "merging (101, 32) into a new token 256\n",
            "merging (116, 104) into a new token 257\n",
            "merging (115, 32) into a new token 258\n",
            "merging (105, 110) into a new token 259\n",
            "merging (116, 32) into a new token 260\n",
            "merging (101, 114) into a new token 261\n",
            "merging (111, 110) into a new token 262\n",
            "merging (32, 97) into a new token 263\n",
            "merging (32, 257) into a new token 264\n",
            "merging (116, 105) into a new token 265\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "merged_vocab"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXGcGkGhNdUt",
        "outputId": "37afe091-357b-4ef0-fdbe-8ac5a8dd56bb"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{(101, 32): 256,\n",
              " (240, 159): 257,\n",
              " (226, 128): 258,\n",
              " (105, 110): 259,\n",
              " (115, 32): 260,\n",
              " (97, 110): 261,\n",
              " (116, 104): 262,\n",
              " (257, 133): 263,\n",
              " (257, 135): 264,\n",
              " (97, 114): 265}"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"tokens length:\", len(tokens))\n",
        "print(\"ids length:\", len(ids))\n",
        "print(f\"compression ratio: {len(tokens) / len(ids):.2f}X\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MhDDjKxrRusF",
        "outputId": "6f3c1672-c43d-423a-c59e-2e3eb1d3beb3"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tokens length: 11308\n",
            "ids length: 11308\n",
            "compression ratio: 1.00X\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = {idx: bytes([idx]) for idx in range(256)}\n",
        "for (p0, p1), idx in merged_vocab.items():\n",
        "    vocab[idx] = vocab[p0] + vocab[p1]"
      ],
      "metadata": {
        "id": "FTv8t2-hV7C5"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Decoding.\n",
        "\n",
        "Given a string of integers in the range [0, vocab_size], what is the text?"
      ],
      "metadata": {
        "id": "U8ZWGjbJVBpf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def decode(ids):\n",
        "    # given ids (list of integers), return Python string\n",
        "    tokens = b\"\".join(vocab[idx] for idx in ids)\n",
        "    text = tokens.decode(\"utf-8\", errors=\"replace\")\n",
        "    return text\n",
        "\n",
        "print(decode([97]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gT5eFwxvVAnb",
        "outputId": "39914307-4d76-430c-8e0f-6e2b3e43edc0"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "a\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Encoding\n",
        "\n",
        "Given a string what are the token?"
      ],
      "metadata": {
        "id": "0ZSfSukeWl5S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(text):\n",
        "    #given python string. return tokens (list of integers)\n",
        "\n",
        "    tokens = list(text.encode(\"utf-8\"))\n",
        "    while len(tokens) >= 2:\n",
        "        stats = get_stats(tokens)\n",
        "        pair = min(stats, key=lambda p: merged_vocab.get(p, float(\"inf\")))\n",
        "        if pair not in merged_vocab:\n",
        "            break\n",
        "        idx = merged_vocab[pair]\n",
        "        tokens = merge(tokens, pair, idx)\n",
        "\n",
        "    return tokens\n",
        "\n",
        "print(encode(\"g\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j4fAUugLWlOD",
        "outputId": "ac3b2ab2-a761-442a-d1c7-240c39f759e4"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[103]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(decode(encode(\"being a programmer\")))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZXYgedIXYvU1",
        "outputId": "40d4b626-b8c5-4117-b57c-963a3d990cc9"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "being a programmer\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using gpt2 Tokenizer"
      ],
      "metadata": {
        "id": "KQ1yw2nkZ-B7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tiktoken\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mj1DAJjxaGMS",
        "outputId": "08d45ce6-08db-4232-c818-98a61b675519"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting tiktoken\n",
            "  Downloading tiktoken-0.6.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2023.12.25)\n",
            "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken) (2.31.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken) (2024.2.2)\n",
            "Installing collected packages: tiktoken\n",
            "Successfully installed tiktoken-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tiktoken\n",
        "\n",
        "# GPT-2 (does not merge spaces)\n",
        "enc = tiktoken.get_encoding(\"gpt2\")\n",
        "print(enc.encode(\"    being a programmer!!!\"))\n",
        "\n",
        "# GPT-4 (merges spaces)\n",
        "enc = tiktoken.get_encoding(\"cl100k_base\")\n",
        "print(enc.encode(\"    being a programmer!!!\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zFPVoW5EfcaV",
        "outputId": "dca1e6fe-5341-492f-c15f-53d8b4e93f42"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[220, 220, 220, 852, 257, 24292, 10185]\n",
            "[262, 1694, 264, 48888, 12340]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Using GPT-4 Tokenizer"
      ],
      "metadata": {
        "id": "NFbKSIZ9gWM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "print(enc.encode(\"–ø—Ä–∏–≤–µ—Ç üëã (hello in Russian!)\"))\n",
        "print(enc.decode(enc.encode(\"–ø—Ä–∏–≤–µ—Ç üëã (hello in Russian!)\")) == \"–ø—Ä–∏–≤–µ—Ç üëã (hello in Russian!)\")\n",
        "# match the above for your own tokenizer, and also implement a train() function"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rm3r1IYMf3yV",
        "outputId": "e3a019c2-e40b-4ef9-da7d-8b3cf772e1fe"
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8164, 2233, 28089, 8341, 62904, 233, 320, 15339, 304, 8690, 16715]\n",
            "True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/karpathy/minbpe.git\n",
        "%cd minbpe"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OQidbvHyiZdX",
        "outputId": "b181e875-6f2a-4307-8fe0-328acd37b8b7"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'minbpe'...\n",
            "remote: Enumerating objects: 217, done.\u001b[K\n",
            "remote: Counting objects: 100% (216/216), done.\u001b[K\n",
            "remote: Compressing objects: 100% (90/90), done.\u001b[K\n",
            "remote: Total 217 (delta 130), reused 187 (delta 120), pack-reused 1\u001b[K\n",
            "Receiving objects: 100% (217/217), 333.18 KiB | 7.75 MiB/s, done.\n",
            "Resolving deltas: 100% (130/130), done.\n",
            "/content/minbpe\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This class is a light wrapper around the RegexTokenizer (2, above) that exactly reproduces the tokenization of GPT-4 in the tiktoken library. The wrapping handles some details around recovering the exact merges in the tokenizer, and the handling of some unfortunate (and likely historical?) 1-byte token permutations."
      ],
      "metadata": {
        "id": "Sds36C68j06j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from minbpe import GPT4Tokenizer\n",
        "\n",
        "tokenizer = GPT4Tokenizer()\n",
        "print(tokenizer.encode(\"–ø—Ä–∏–≤–µ—Ç üëã (hello in Russian!)\"))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Li7YTtr2ivY-",
        "outputId": "87e1b0d5-c7e0-4c65-bc0c-6c1c08afc7de"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[8164, 2233, 28089, 8341, 62904, 233, 320, 15339, 304, 8690, 16715]\n"
          ]
        }
      ]
    }
  ]
}